{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.SQL\n",
    "\n",
    "Data/Tools:\n",
    "\n",
    "* Hive User Guide: https://cwiki.apache.org/confluence/display/Hive/GettingStarted\n",
    "* Hive ORC: http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html\n",
    "* Hive Parquet: http://www.cloudera.com/content/cloudera/en/documentation/cdh5/v5-0-0/CDH5-Installation-Guide/cdh5ig_parquet.html\n",
    "\n",
    "\n",
    "5.1 Create a Spark-SQL table for the NASA Log files! Use either python or awk to convert the log file to a structured format (CSV) that is manageable by Hive! Use the text format for the table definition!\n",
    "\n",
    "     cat /data/NASA_access_log_Jul95 |awk -F' ' '{print \"\\\"\"$4 $5\"\\\",\"$(NF-1)\",\"$(NF)}' > nasa.csv\n",
    "\n",
    "\n",
    "5.2 Run an SQL query that outputs the number of occurrences of each HTTP response code!\n",
    "\n",
    "5.3 Based on the initially created table define an ORC and Parquet-based table. Repeat the query!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f5c93cc207f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m on_time_dataframe = spark.read.format('com.databricks.spark.csv')  .options(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtreatEmptyValuesAsNulls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   )\\\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/On_Time_On_Time_Performance_2015.csv.bz2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "on_time_dataframe = spark.read.format('com.databricks.spark.csv')\\\n",
    "  .options(\n",
    "    header='true',\n",
    "    treatEmptyValuesAsNulls='true',\n",
    "  )\\\n",
    "  .load('data/On_Time_On_Time_Performance_2015.csv.bz2')\n",
    "on_time_dataframe.registerTempTable(\"on_time_performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_cast_performance = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  Year, Quarter, Month, DayofMonth, DayOfWeek, FlightDate,\n",
    "  Carrier, TailNum, FlightNum,\n",
    "  Origin, OriginCityName, OriginState,\n",
    "  Dest, DestCityName, DestState,\n",
    "  DepTime, cast(DepDelay as float), cast(DepDelayMinutes as int),\n",
    "  cast(TaxiOut as float), cast(TaxiIn as float),\n",
    "  WheelsOff, WheelsOn,\n",
    "  ArrTime, cast(ArrDelay as float), cast(ArrDelayMinutes as float),\n",
    "  cast(Cancelled as int), cast(Diverted as int),\n",
    "  cast(ActualElapsedTime as float), cast(AirTime as float),\n",
    "  cast(Flights as int), cast(Distance as float),\n",
    "  cast(CarrierDelay as float), cast(WeatherDelay as float), cast(NASDelay as float),\n",
    "  cast(SecurityDelay as float), cast(LateAircraftDelay as float),\n",
    "  CRSDepTime, CRSArrTime\n",
    "FROM\n",
    "  on_time_performance\n",
    "\"\"\")\n",
    "\n",
    "# Replace on_time_performance table with our new, trimmed table and show its contents\n",
    "trimmed_cast_performance.registerTempTable(\"on_time_performance\")\n",
    "trimmed_cast_performance.show()\n",
    "\n",
    "# Verify we can sum numeric columns\n",
    "spark.sql(\"\"\"SELECT\n",
    "  SUM(WeatherDelay), SUM(CarrierDelay), SUM(NASDelay),\n",
    "  SUM(SecurityDelay), SUM(LateAircraftDelay)\n",
    "FROM on_time_performance\n",
    "\"\"\").show()\n",
    "\n",
    "# Save records as gzipped json lines\n",
    "trimmed_cast_performance.toJSON()\\\n",
    "  .saveAsTextFile(\n",
    "    'data/on_time_performance.jsonl.gz',\n",
    "    'org.apache.hadoop.io.compress.GzipCodec'\n",
    "  )\n",
    "\n",
    "# View records on filesystem\n",
    "# gunzip -c data/on_time_performance.jsonl.gz/part-00000.gz | head\n",
    "\n",
    "# Save records using Parquet\n",
    "trimmed_cast_performance.write.mode(\"overwrite\").parquet(\"data/on_time_performance.parquet\")\n",
    "\n",
    "# Load JSON records back\n",
    "on_time_dataframe = spark.read.json('data/on_time_performance.jsonl.gz')\n",
    "on_time_dataframe.show()\n",
    "\n",
    "# Load the parquet file back\n",
    "on_time_dataframe = spark.read.parquet('data/on_time_performance.parquet')\n",
    "on_time_dataframe.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
