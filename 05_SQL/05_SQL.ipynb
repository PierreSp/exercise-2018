{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.SQL and Dataframes\n",
    "\n",
    "References:\n",
    "\n",
    "* Spark-SQL, <https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes>\n",
    "\n",
    "\n",
    "# 5.1  Example Walkthrough\n",
    "Follow the Spark SQL and Dataframes Examples below!\n",
    "\n",
    "### Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initiated...\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark\n",
    "APP_NAME = \"PySpark Lecture\"\n",
    "SPARK_MASTER=\"spark://mpp3r03c04s04.cos.lrz.de:7077\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "    sc and spark\n",
    "except NameError as e:\n",
    "  #import findspark\n",
    "  #findspark.init()\n",
    "    import pyspark\n",
    "    import pyspark.sql\n",
    "    from pyspark.sql import Row\n",
    "    conf=pyspark.SparkConf().set(\"spark.cores.max\", \"8\")\n",
    "    sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n",
    "    spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Russell Jurney', u'Relato', u'CEO'],\n",
       " [u'Florian Liebert', u'Mesosphere', u'CEO'],\n",
       " [u'Don Brown', u'Rocana', u'CIO'],\n",
       " [u'Steve Jobs', u'Apple', u'CEO'],\n",
       " [u'Donald Trump', u'The Trump Organization', u'CEO'],\n",
       " [u'Russell Jurney', u'Data Syndrome', u'Principal Consultant']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"../data/example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Rows\n",
    "\n",
    "Creating `pyspark.sql.Rows` out of your data so you can create DataFrames..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the CSV into a pyspark.sql.Row\n",
    "def csv_to_row(line):\n",
    "    parts = line.split(\",\")\n",
    "    row = Row(\n",
    "      name=parts[0],\n",
    "      company=parts[1],\n",
    "      title=parts[2]\n",
    "    )\n",
    "    return row\n",
    "\n",
    "# Apply the function to get rows in an RDD\n",
    "rows = csv_lines.map(csv_to_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames from RDDs\n",
    "\n",
    "Using the `RDD.toDF()` method to create a dataframe, registering the `DataFrame` as a temporary table with Spark SQL, and counting the jobs per person using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           name|total|\n",
      "+---------------+-----+\n",
      "|   Donald Trump|    1|\n",
      "|Florian Liebert|    1|\n",
      "|      Don Brown|    1|\n",
      "| Russell Jurney|    2|\n",
      "|     Steve Jobs|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name=u'Donald Trump', total=1),\n",
       " Row(name=u'Florian Liebert', total=1),\n",
       " Row(name=u'Don Brown', total=1),\n",
       " Row(name=u'Russell Jurney', total=2),\n",
       " Row(name=u'Steve Jobs', total=1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to a pyspark.sql.DataFrame\n",
    "rows_df = rows.toDF()\n",
    "\n",
    "# Register the DataFrame for Spark SQL\n",
    "rows_df.registerTempTable(\"executives\")\n",
    "\n",
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_counts = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  name,\n",
    "  COUNT(*) AS total\n",
    "  FROM executives\n",
    "  GROUP BY name\n",
    "\"\"\")\n",
    "job_counts.show()\n",
    "\n",
    "# Go back to an RDD\n",
    "job_counts.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2-5.4 NASA DataSet\n",
    "\n",
    "5.2 Create a Spark-SQL table with fields for IP/Host and Response Code from the NASA Log file! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.2 ms, sys: 22.1 ms, total: 62.3 ms\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import Row\n",
    "nasa_lines = sc.textFile(\"../data/nasa/NASA_access_log_Jul95\")\n",
    "spark_dataframe=nasa_lines.map(lambda a: Row(host=a.split()[0], response_code=a.split()[-2] if len(a.split())>2 else \"No Value\")).toDF()\n",
    "spark_dataframe.registerTempTable(\"nasa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Run an SQL query that outputs the number of occurrences of each HTTP response code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 82.4 ms, sys: 38 ms, total: 120 ms\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = spark.sql(\"\"\"select response_code, count(*) as count from nasa group by response_code\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Cachen Sie den Dataframe und führen Sie dieselbe Query nochmals aus! Messen Sie die Laufzeit für das Cachen und für die Ausführungszeit der Query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 ms, sys: 2.96 ms, total: 19.7 ms\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark_dataframe.cache()\n",
    "spark_dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98.5 ms, sys: 29.6 ms, total: 128 ms\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = spark.sql(\"\"\"select response_code, count(*) as count from nasa group by response_code\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5 Führen Sie diesselbe Query mit/ohne Cache und 8, 16 Cores aus! Dokumentieren und erklären Sie das Ergebnis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6 Convert the output to a Pandas dataframe and calculate the percentage of total for each response code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_code</th>\n",
       "      <th>count</th>\n",
       "      <th>counts_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>1701534</td>\n",
       "      <td>89.946636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>302</td>\n",
       "      <td>46573</td>\n",
       "      <td>2.461946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>501</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>404</td>\n",
       "      <td>10845</td>\n",
       "      <td>0.573289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>403</td>\n",
       "      <td>54</td>\n",
       "      <td>0.002855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>62</td>\n",
       "      <td>0.003277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>304</td>\n",
       "      <td>132627</td>\n",
       "      <td>7.010940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No Value</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response_code    count  counts_pct\n",
       "0           200  1701534   89.946636\n",
       "1           302    46573    2.461946\n",
       "2           501       14    0.000740\n",
       "3           404    10845    0.573289\n",
       "4           403       54    0.002855\n",
       "5           500       62    0.003277\n",
       "6           304   132627    7.010940\n",
       "7      No Value        1    0.000053\n",
       "8           400        5    0.000264"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"counts_pct\"]=(results[\"count\"]/results[\"count\"].sum()*100) \n",
    "results"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
