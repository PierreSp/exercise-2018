{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.SQL and Dataframes\n",
    "\n",
    "References:\n",
    "\n",
    "* Spark-SQL, >https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes>\n",
    "\n",
    "\n",
    "5.1 Create a Spark-SQL table for the NASA Log files! Use either python or awk to convert the log file to a structured format (CSV) that is manageable by Hive! Use the text format for the table definition!\n",
    "\n",
    "     cat /data/NASA_access_log_Jul95 |awk -F' ' '{print \"\\\"\"$4 $5\"\\\",\"$(NF-1)\",\"$(NF)}' > nasa.csv\n",
    "\n",
    "\n",
    "5.2 Run an SQL query that outputs the number of occurrences of each HTTP response code!\n",
    "\n",
    "\n",
    "Initialize a Spark Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for creating Dataframe and SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<code goes here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Load the Flight On Time Performance into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_time_dataframe = spark.read.format('com.databricks.spark.csv')\\\n",
    "  .options(\n",
    "    header='true',\n",
    "    treatEmptyValuesAsNulls='true',\n",
    "  )\\\n",
    "  .load('../data/On_Time_On_Time_Performance_2015.csv.bz2')\n",
    "on_time_dataframe.registerTempTable(\"on_time_performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_cast_performance = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  Year, Quarter, Month, DayofMonth, DayOfWeek, FlightDate,\n",
    "  Carrier, TailNum, FlightNum,\n",
    "  Origin, OriginCityName, OriginState,\n",
    "  Dest, DestCityName, DestState,\n",
    "  DepTime, cast(DepDelay as float), cast(DepDelayMinutes as int),\n",
    "  cast(TaxiOut as float), cast(TaxiIn as float),\n",
    "  WheelsOff, WheelsOn,\n",
    "  ArrTime, cast(ArrDelay as float), cast(ArrDelayMinutes as float),\n",
    "  cast(Cancelled as int), cast(Diverted as int),\n",
    "  cast(ActualElapsedTime as float), cast(AirTime as float),\n",
    "  cast(Flights as int), cast(Distance as float),\n",
    "  cast(CarrierDelay as float), cast(WeatherDelay as float), cast(NASDelay as float),\n",
    "  cast(SecurityDelay as float), cast(LateAircraftDelay as float),\n",
    "  CRSDepTime, CRSArrTime\n",
    "FROM\n",
    "  on_time_performance\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace on_time_performance table with our new, trimmed table and show its contents\n",
    "\n",
    "trimmed_cast_performance.show()\n",
    "\n",
    "\n",
    "\n",
    "# View records on filesystem\n",
    "# gunzip -c data/on_time_performance.jsonl.gz/part-00000.gz | head\n",
    "\n",
    "\n",
    "\n",
    "# Load the parquet file back\n",
    "on_time_dataframe = spark.read.parquet('data/on_time_performance.parquet')\n",
    "on_time_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we can sum numeric columns\n",
    "trimmed_cast_performance.registerTempTable(\"on_time_performance\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT\n",
    "  SUM(WeatherDelay), SUM(CarrierDelay), SUM(NASDelay),\n",
    "  SUM(SecurityDelay), SUM(LateAircraftDelay)\n",
    "FROM on_time_performance\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write files in different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save records as gzipped json lines\n",
    "trimmed_cast_performance.toJSON()\\\n",
    "  .saveAsTextFile(\n",
    "    'data/on_time_performance.jsonl.gz',\n",
    "    'org.apache.hadoop.io.compress.GzipCodec'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save records using Parquet\n",
    "trimmed_cast_performance.write.mode(\"overwrite\").parquet(\"data/on_time_performance.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON records back\n",
    "on_time_dataframe = spark.read.json('data/on_time_performance.jsonl.gz')\n",
    "on_time_dataframe.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
