{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spark\n",
    "\n",
    "Spark Programming Guide: <https://spark.apache.org/docs/latest/> (use Python API recommended)\n",
    "Spark API: <https://spark.apache.org/docs/latest/api/python/index.html>\n",
    "\n",
    "\n",
    "# 3.1 Example Walkthrough\n",
    "3.1 Follow the Spark Examples below! After completion see Exercise 3.2 and 3.3!\n",
    "\n",
    "\n",
    "### Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initiated...\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark\n",
    "#SPARK_MASTER=\"local[1]\"\n",
    "SPARK_MASTER=\"spark://mpp3r03c04s06.cos.lrz.de:7077\"\n",
    "APP_NAME = \"PySpark Lecture\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "    sc and spark\n",
    "except NameError as e:\n",
    "  #import findspark\n",
    "  #findspark.init()\n",
    "    import pyspark\n",
    "    import pyspark.sql\n",
    "    sc = pyspark.SparkContext(master=SPARK_MASTER)\n",
    "    spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Russell Jurney', u'Relato', u'CEO'],\n",
       " [u'Florian Liebert', u'Mesosphere', u'CEO'],\n",
       " [u'Don Brown', u'Rocana', u'CIO'],\n",
       " [u'Steve Jobs', u'Apple', u'CEO'],\n",
       " [u'Donald Trump', u'The Trump Organization', u'CEO'],\n",
       " [u'Russell Jurney', u'Data Syndrome', u'Principal Consultant']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"../data/example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Objects from CSV\n",
    "\n",
    "Using a function with a map operation to create objects (dicts) as records..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the CSV lines into objects\n",
    "def csv_to_record(line):\n",
    "    parts = line.split(\",\")\n",
    "    record = {\n",
    "      \"name\": parts[0],\n",
    "      \"company\": parts[1],\n",
    "      \"title\": parts[2]\n",
    "    }\n",
    "    return record\n",
    "\n",
    "# Apply the function to every record\n",
    "records = csv_lines.map(csv_to_record)\n",
    "\n",
    "# Inspect the first item in the dataset\n",
    "records.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy\n",
    "\n",
    "Using the groupBy operator to count the number of jobs per person..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the records by the name of the person\n",
    "grouped_records = records.groupBy(lambda x: x[\"name\"])\n",
    "\n",
    "# Show the first group\n",
    "grouped_records.first()\n",
    "\n",
    "# Count the groups\n",
    "job_counts = grouped_records.map(\n",
    "  lambda x: {\n",
    "    \"name\": x[0],\n",
    "    \"job_count\": len(x[1])\n",
    "  }\n",
    ")\n",
    "\n",
    "job_counts.first()\n",
    "\n",
    "job_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map vs FlatMap\n",
    "\n",
    "Understanding the difference between the map and flatmap operators..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a relation of words by line\n",
    "words_by_line = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\n",
    "\n",
    "print(words_by_line.collect())\n",
    "\n",
    "# Compute a relation of words\n",
    "flattened_words = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\\\n",
    "  .flatMap(lambda x: x)\n",
    "\n",
    "flattened_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Further Exercises\n",
    "\n",
    "3.2 Implement a wordcount using Spark. Who many words are in the file `example.csv`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = csv_lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 How many log enteries per HTTP Response Code exist? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file using the SparkContext\n",
    "nasa_lines = sc.textFile(\"../data/nasa/NASA_access_log_Jul95\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_lines.map(lambda a: (a.split()[-2] if len(a.split())>2 else \"No Value\", 1))\\\n",
    "          .reduceByKey(lambda a, b: a + b)\\\n",
    "          .collect()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
