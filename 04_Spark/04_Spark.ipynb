{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spark\n",
    "\n",
    "Spark Programming Guide: <https://spark.apache.org/docs/latest/> (use Python API recommended)\n",
    "Spark API: <https://spark.apache.org/docs/latest/api/python/index.html>\n",
    "\n",
    "        \n",
    "3.1 Follow the Spark Examples below! After completion see Exercise 3.2 and 3.3!\n",
    "\n",
    "\n",
    "## Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initiated...\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark\n",
    "APP_NAME = \"PySpark Lecture\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "    sc and spark\n",
    "except NameError as e:\n",
    "  #import findspark\n",
    "  #findspark.init()\n",
    "    import pyspark\n",
    "    import pyspark.sql\n",
    "    sc = pyspark.SparkContext()\n",
    "    spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Russell Jurney', u'Relato', u'CEO'],\n",
       " [u'Florian Liebert', u'Mesosphere', u'CEO'],\n",
       " [u'Don Brown', u'Rocana', u'CIO'],\n",
       " [u'Steve Jobs', u'Apple', u'CEO'],\n",
       " [u'Donald Trump', u'The Trump Organization', u'CEO'],\n",
       " [u'Russell Jurney', u'Data Syndrome', u'Principal Consultant']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"../data/example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Objects from CSV\n",
    "\n",
    "Using a function with a map operation to create objects (dicts) as records..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': u'Relato', 'name': u'Russell Jurney', 'title': u'CEO'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the CSV lines into objects\n",
    "def csv_to_record(line):\n",
    "  parts = line.split(\",\")\n",
    "  record = {\n",
    "    \"name\": parts[0],\n",
    "    \"company\": parts[1],\n",
    "    \"title\": parts[2]\n",
    "  }\n",
    "  return record\n",
    "\n",
    "# Apply the function to every record\n",
    "records = csv_lines.map(csv_to_record)\n",
    "\n",
    "# Inspect the first item in the dataset\n",
    "records.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy\n",
    "\n",
    "Using the groupBy operator to count the number of jobs per person..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'job_count': 1, 'name': u'Donald Trump'},\n",
       " {'job_count': 1, 'name': u'Don Brown'},\n",
       " {'job_count': 1, 'name': u'Florian Liebert'},\n",
       " {'job_count': 1, 'name': u'Steve Jobs'},\n",
       " {'job_count': 2, 'name': u'Russell Jurney'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the records by the name of the person\n",
    "grouped_records = records.groupBy(lambda x: x[\"name\"])\n",
    "\n",
    "# Show the first group\n",
    "grouped_records.first()\n",
    "\n",
    "# Count the groups\n",
    "job_counts = grouped_records.map(\n",
    "  lambda x: {\n",
    "    \"name\": x[0],\n",
    "    \"job_count\": len(x[1])\n",
    "  }\n",
    ")\n",
    "\n",
    "job_counts.first()\n",
    "\n",
    "job_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map vs FlatMap\n",
    "\n",
    "Understanding the difference between the map and flatmap operators..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'Russell Jurney', u'Relato', u'CEO'], [u'Florian Liebert', u'Mesosphere', u'CEO'], [u'Don Brown', u'Rocana', u'CIO'], [u'Steve Jobs', u'Apple', u'CEO'], [u'Donald Trump', u'The Trump Organization', u'CEO'], [u'Russell Jurney', u'Data Syndrome', u'Principal Consultant']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'Russell Jurney',\n",
       " u'Relato',\n",
       " u'CEO',\n",
       " u'Florian Liebert',\n",
       " u'Mesosphere',\n",
       " u'CEO',\n",
       " u'Don Brown',\n",
       " u'Rocana',\n",
       " u'CIO',\n",
       " u'Steve Jobs',\n",
       " u'Apple',\n",
       " u'CEO',\n",
       " u'Donald Trump',\n",
       " u'The Trump Organization',\n",
       " u'CEO',\n",
       " u'Russell Jurney',\n",
       " u'Data Syndrome',\n",
       " u'Principal Consultant']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a relation of words by line\n",
    "words_by_line = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\n",
    "\n",
    "print(words_by_line.collect())\n",
    "\n",
    "# Compute a relation of words\n",
    "flattened_words = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\\\n",
    "  .flatMap(lambda x: x)\n",
    "\n",
    "flattened_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Further Exercises\n",
    "\n",
    "3.2 Implement a wordcount using Spark. Make sure that you only allocate 1 core for the interactive Spark shell:\n",
    "\n",
    "        pyspark --total-executor-cores 1 --master spark://<hostname>\n",
    "\n",
    "\n",
    "3.3 Implement the NASA log file analysis using Spark! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
